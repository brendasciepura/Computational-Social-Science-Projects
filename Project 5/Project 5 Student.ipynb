{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Computational Social Science] Project 5: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Student: Brenda Sciepura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will use natural language processing techniques to explore a dataset containing tweets from members of the 116th United States Congress that met from January 3, 2019 to January 2, 2021. The dataset has also been cleaned to contain information about each legislator. Concretely, you will do the following:\n",
    "\n",
    "* Preprocess the text of legislators' tweets\n",
    "* Conduct Exploratory Data Analysis of the text\n",
    "* Use sentiment analysis to explore differences between legislators' tweets\n",
    "* Featurize text with manual feature engineering, frequency-based, and vector-based techniques\n",
    "* Predict legislators' political parties and whether they are a Senator or Representative\n",
    "\n",
    "You will explore two questions that relate to two central findings in political science and examine how they relate to the text of legislators' tweets. First, political scientists have argued that U.S. politics is currently highly polarized relative to other periods in American history, but also that the polarization is asymmetric. Historically, there were several conservative Democrats (i.e. \"blue dog Democrats\") and liberal Republicans (i.e. \"Rockefeller Republicans\"), as measured by popular measurement tools like [DW-NOMINATE](https://en.wikipedia.org/wiki/NOMINATE_(scaling_method)#:~:text=DW\\%2DNOMINATE\\%20scores\\%20have\\%20been,in\\%20the\\%20liberal\\%2Dconservative\\%20scale.). However, in the last few years, there are few if any examples of any Democrat in Congress being further to the right than any Republican and vice versa. At the same time, scholars have argued that this polarization is mostly a function of the Republican party moving further right than the Democratic party has moved left. **Does this sort of asymmetric polarization show up in how politicians communicate to their constituents through tweets?**\n",
    "\n",
    "Second, the U.S. Congress is a bicameral legislature, and there has long been debate about partisanship in the Senate versus the House. The House of Representatives is apportioned by population and all members serve two year terms. In the Senate, each state receives two Senators and each Senator serves a term of six years. For a variety of reasons (smaller chamber size, more insulation from the voters, rules and norms like the filibuster, etc.), the Senate has been argued to be the \"cooling saucer\" of Congress in that it is more bipartisan and moderate than the House. **Does the theory that the Senate is more moderate have support in Senators' tweets?**\n",
    "\n",
    "**Note**: See the project handout for more details on caveats and the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download libraries\n",
    "# ----------\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install scattertext\n",
    "#!pip install wordcloud\n",
    "#!pip install textblob\n",
    "#!pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# countvectorizer, tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "# textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "## LDA\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# load wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# for kMeans and silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# for the classification task \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# load libraires\n",
    "import random\n",
    "from adjustText import adjust_text\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# load STOP_WORDS module from spaCy library\n",
    "# ----------\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# load punctuation module from string library\n",
    "# ----------\n",
    "from string import punctuation  # note that this is a base library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "# ----------\n",
    "congress_tweets = pd.read_csv(\"data/116th Congressional Tweets and Demographics.csv\")\n",
    "congress_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with text data is to preprocess it. Make sure you do the following:\n",
    "\n",
    "* Remove punctuation and stop words. The `rem_punc_stop()` function we used in lab is provided to you but you should feel free to edit it as necessary for other steps\n",
    "* Remove tokens that occur frequently in tweets, but may not be helpful for downstream classification. For instance, many tweets contain a flag for retweeting, or share a URL \n",
    "\n",
    "As you search online, you might run into solutions that rely on regular expressions. You are free to use these, but you should also be able to preprocess using the techniques we covered in lab. Specifically, we encourage you to use spaCy's token attributes and string methods to do some of this text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## defining function to remove punctuation, stop words and other pre-defined characters \n",
    "def rem_punc_stop(text):\n",
    "    \n",
    "    ## tokenize\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ## extract tokens (not including tokens if they are stop words, spaces or punctuation, and I'm also removing any alpha numeric characters)\n",
    "    spacy_words = [token.text for token in doc if not token.is_stop and not token.is_space and not token.is_punct and (token.is_alpha or token.is_digit)]\n",
    "    \n",
    "    ## removing URL \n",
    "    spacy_words = [word for word in spacy_words if not word.startswith('http')]\n",
    "    \n",
    "    # remove flags for retweeting (e.g., RT)\n",
    "    spacy_words = [word for word in spacy_words if word.lower() != 'rt']\n",
    "    \n",
    "    # remove another characters that appear very frequently in tweets (QT)\n",
    "    spacy_words = [word for word in spacy_words if word.lower() != 'qt']\n",
    "    \n",
    "    # remove \"amp\" which comes after each & in the text (&amp)\n",
    "    spacy_words = [word for word in spacy_words if word.lower() != 'amp']\n",
    "     \n",
    "    return spacy_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## here I was checking why \"amp\" appeared so many times and then I decided to remove it \n",
    "\n",
    "## tweets_with_amp = subset_df[subset_df['text'].str.contains(\"amp\")]\n",
    "## tweets_with_amp_text = tweets_with_amp['text']\n",
    "\n",
    "# Print only the first 20 tweets\n",
    "## for tweet_text in tweets_with_amp_text.head(10): print(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subset the df to 7,000 random rows\n",
    "subset_df = congress_tweets.sample(n=7000)\n",
    "\n",
    "# apply the function to all columns \n",
    "subset_df['tokens'] = subset_df['text'].map(lambda x: rem_punc_stop(x)) \n",
    "subset_df['tokens'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## removing independents from dataset to have a binary outcome\n",
    "subset_df = subset_df[subset_df['party'] != 'Independent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm creating this here so that I can use the tokenized version or the full text version as needed \n",
    "subset_df['tokens_full_text'] = subset_df['tokens'].map(lambda text: ' '.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use two of the techniques we covered in lab (or other techniques outside of lab!) to explore the text of the tweets. You should construct these visualizations with an eye toward the eventual classification tasks: (1) predicting the legislator's political party based on the text of their tweet, and (2) predicting whether the legislator is a Senator or Representative. As a reminder, in lab we covered word frequencies, word clouds, word/character counts, scattertext, and topic modeling as possible exploration tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 1: WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to text object\n",
    "text = ' '.join(subset_df['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# create WordCloud visualization using the \"text\" object by \n",
    "wordcloud = WordCloud(background_color = \"black\",  \n",
    "                      colormap='rainbow',\n",
    "                      random_state=3141).generate(text)              \n",
    "\n",
    "# plot \n",
    "plt.imshow(wordcloud, interpolation = 'bilinear') \n",
    "plt.axis('off')                        \n",
    "plt.show()                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I create two distinct wordclouds: one representing tweets coming from republicans and the other tweets coming from democrats\n",
    "\n",
    "# Filter data for Republicans\n",
    "republican_text = ' '.join(subset_df[subset_df['party'] == 'Republican']['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# Create WordCloud for Republicans\n",
    "republican_wordcloud = WordCloud(background_color=\"black\", \n",
    "                                 colormap='rainbow', \n",
    "                                 random_state=3141).generate(republican_text)\n",
    "\n",
    "# Plot Republican WordCloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(republican_wordcloud, interpolation='bilinear')\n",
    "plt.title('Republicans')\n",
    "plt.axis('off')\n",
    "\n",
    "# Filter data for Democrats\n",
    "democrat_text = ' '.join(subset_df[subset_df['party'] == 'Democrat']['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# Create WordCloud for Democrats\n",
    "democrat_wordcloud = WordCloud(background_color=\"grey\", \n",
    "                               colormap='rainbow', \n",
    "                               random_state=3141).generate(democrat_text)\n",
    "\n",
    "# Plot Democrat WordCloud\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(democrat_wordcloud, interpolation='bilinear')\n",
    "plt.title('Democrats')\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Comments on wordclouds: \n",
    "## It's interesting that \"thank\" is way more used by republicans. And that democrat stands out in republicans' tweets whereas Trump stands out in democrat tweets. And the word \"act\" stands out more in democtats' tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I create two distinct wordclouds: one representing tweets coming from Senators and the other tweets coming from Representatives\n",
    "\n",
    "# Filter data for Representatives\n",
    "representative_text = ' '.join(subset_df[subset_df['position'] == 'Rep']['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# Create WordCloud for Representatives\n",
    "representative_wordcloud = WordCloud(background_color=\"black\", \n",
    "                                 colormap='rainbow', \n",
    "                                 random_state=3141).generate(representative_text)\n",
    "\n",
    "# Plot Representatives WordCloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(representative_wordcloud, interpolation='bilinear')\n",
    "plt.title('Representatives')\n",
    "plt.axis('off')\n",
    "\n",
    "# Filter data for Senators\n",
    "senator_text = ' '.join(subset_df[subset_df['position'] == 'Sen']['tokens'].map(lambda text: ' '.join(text)))\n",
    "\n",
    "# Create WordCloud for Senators\n",
    "senator_wordcloud = WordCloud(background_color=\"grey\", \n",
    "                               colormap='rainbow', \n",
    "                               random_state=3141).generate(senator_text)\n",
    "\n",
    "# Plot Senators WordCloud\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(senator_wordcloud, interpolation='bilinear')\n",
    "plt.title('Senators')\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Comments on wordclouds: \n",
    "## Representatives' graph is closer to Republicans' graph, whereas Senatos' graph is closer to Democrats' graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 2: Word/Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count number of characters -- given that these are tweets and there are rules about lenght maybe this is not so important\n",
    "subset_df['tweet_length'] = subset_df['text'].apply(len)\n",
    "\n",
    "# create histogram of tweet length (number of characters)\n",
    "sns.displot(subset_df, x=\"tweet_length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count number of words\n",
    "subset_df['word_count'] = subset_df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# create histogram of tweet length (number of characters)\n",
    "sns.displot(subset_df, x=\"word_count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 3: Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf matrix\n",
    "# ----------\n",
    "\n",
    "# set X dataset\n",
    "X = subset_df['text']        \n",
    "\n",
    "# initialize tf-idf using our preprocessing function\n",
    "tf = TfidfVectorizer(tokenizer = rem_punc_stop, # use our function for tokenizing created above\n",
    "                     token_pattern = None)      # set to \"None\" since we have specify our own pattern\n",
    "\n",
    "# fit and transform data\n",
    "tfidf_matrix = tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert matrix to an arrray and then to a dataframe\n",
    "tfidf_df = pd.DataFrame(data = tfidf_matrix.toarray(),       # convert to array than to datafram\n",
    "                         columns=tf.get_feature_names_out()) # specify column names as feature names from TF vectorizer\n",
    "\n",
    "# sort by term frequency on the first document\n",
    "tfidf_df.T.nlargest(10,  # transpose the matrix = columns become documents and rows are words\n",
    "                     0)  # on column index 0 to show the largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "# ------------------------------\n",
    "\n",
    "# implement kmeans clustering\n",
    "# ----------\n",
    "kmeans = KMeans(n_clusters=3,       # specify # of clusters\n",
    "                max_iter=300        # specify # of iterations\n",
    "                ).fit(tfidf_matrix) # specify data to fit\n",
    "\n",
    "# append labels to dataframe\n",
    "# ----------\n",
    "subset_df['cluster']= kmeans.labels_    # add labels to original data frame\n",
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# determining optional k: elbow method\n",
    "# ------------------------------\n",
    "\n",
    "# settings\n",
    "# ---------\n",
    "# set an empty list \n",
    "Sum_of_squared_distances = []\n",
    "\n",
    "# set range of k\n",
    "K = range(1, 10) # second number is the ceiling of the range, but remember it is exclusive\n",
    "\n",
    "\n",
    "# loop over k\n",
    "# ---------\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k,     # number of clusters\n",
    "                init='k-means++', # method for initalization \n",
    "                n_init=10)        # number of times the k-means algorithm is run with different centroid seeds\n",
    "    km = km.fit(tfidf_matrix)     # fit\n",
    "    Sum_of_squared_distances.append(km.inertia_) # pipe inertia calculations into list\n",
    "\n",
    "\n",
    "# plot results\n",
    "# ---------\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.xticks(range(1, max(K) + 1, 1))\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "## the less steep drop happens at 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# determining optional k: silhouette scores\n",
    "# ------------------------------\n",
    "\n",
    "# iterate over a k-means fits to have different clusters\n",
    "# ---------\n",
    "def run_KMeans(max_k, data):\n",
    "    max_k += 1\n",
    "    kmeans_results = dict()\n",
    "    for k in range(2 , max_k):\n",
    "        kmeans = KMeans(n_clusters = k\n",
    "                               , init = 'k-means++'\n",
    "                               , n_init = 10\n",
    "                               , random_state = 1)\n",
    "\n",
    "        kmeans_results.update( {k : kmeans.fit(data)} )\n",
    "        \n",
    "    return kmeans_results\n",
    "\n",
    "\n",
    "# calculate average silhouettes scores \n",
    "# ---------\n",
    "# plot silhouettes scores\n",
    "def printAvg(avg_dict):\n",
    "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
    "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
    "\n",
    "# plot silhouettes scores       \n",
    "def plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg): \n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax1.set_xlim([-0.2, 1])   # play with this to set x-axis limits\n",
    "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.title((\"Silhouette analysis for K = %d\" % n_clusters), fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = 10\n",
    "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
    "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
    "    plt.show()\n",
    "    \n",
    "# put it altogether\n",
    "def silhouette(kmeans_dict, df, plot=True):\n",
    "    df = df.to_numpy()\n",
    "    avg_dict = dict()\n",
    "    for n_clusters, kmeans in kmeans_dict.items():      \n",
    "        kmeans_labels = kmeans.predict(df)\n",
    "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
    "        avg_dict.update( {silhouette_avg : n_clusters} )\n",
    "    \n",
    "        if(plot): plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate silhouette scores across potential k-means clusters \n",
    "# ---------\n",
    "\n",
    "# set K\n",
    "k = 6 # choose 7 based on the elbow method result from above\n",
    "\n",
    "# run the k-means algorithm\n",
    "kmeans_results = run_KMeans(k,                # set k\n",
    "                            data = tfidf_df)  # identify data\n",
    "\n",
    "\n",
    "# plot the silhouette analysis\n",
    "silhouette(kmeans_results,     # take k-means results\n",
    "           tfidf_df)           # use the dataframe version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster analysis of top words in each cluster\n",
    "# ------------------------------\n",
    "\n",
    "# get the top features from each cluster\n",
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # indices for each cluster\n",
    "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = tf.get_feature_names_out()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "# plot them on a barplot\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run k-means and visualize word count\n",
    "# ---------\n",
    "\n",
    "# get results from 5 clusters \n",
    "best_result = 5\n",
    "kmeans = kmeans_results.get(best_result)\n",
    "\n",
    "# processing for plot\n",
    "tfidf_array = tfidf_df.to_numpy()     # convert dataframe to array\n",
    "prediction = kmeans.predict(tfidf_df) # predict cluster using tf-idf dataframe\n",
    "\n",
    "\n",
    "# plot\n",
    "n_feats = 20\n",
    "dfs = get_top_features_cluster(tfidf_array, # specify dataset which is an array\n",
    "                               prediction,  # make specify prediciton\n",
    "                               n_feats )    # set number of features \n",
    "plotWords(dfs, # specify data for plotting  \n",
    "          13)  # set number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a function to print the top words that we'll use in our model\n",
    "# --------\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "# ---------------------------------------\n",
    "\n",
    "# pre-processing\n",
    "# --------\n",
    "# create a new data object called X\n",
    "X = subset_df['tokens_full_text']\n",
    "\n",
    "# initialize tf-idf function and set parameters\n",
    "tf = TfidfVectorizer(tokenizer = rem_punc_stop,  # specify our function for remove punc and stop words\n",
    "                     token_pattern = None)       # specify \"None\" to remove warning. Is this necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply tf-idf vectorizer to our data (X)\n",
    "tfidf_matrix = tf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify the output to be a dense matrix\n",
    "dense_matrix = tfidf_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intitialize LDA model and \n",
    "# --------\n",
    "# initialize LDA and set model parameters\n",
    "lda = LatentDirichletAllocation(n_components=5, # specify the number of components\n",
    "                                max_iter=20,    # specify the number of iterations \n",
    "                                random_state=0) # set a seed for reproducibility\n",
    "\n",
    "# fit LDA model to our dense matrix\n",
    "lda = lda.fit(np.asarray(dense_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# post-processing\n",
    "# --------\n",
    "# get feature names from our tf-idf vector\n",
    "tf_feature_names = tf.get_feature_names_out()\n",
    "\n",
    "# print top words \n",
    "print_top_words(lda,               # specify model\n",
    "                tf_feature_names,  # specify feature names vector\n",
    "                20)                # specify how many words we want to see\n",
    " \n",
    "\n",
    "# now transform our data using the lda model and create a dataframe\n",
    "topic_dist = lda.transform(tfidf_matrix)\n",
    "topic_dist_df = pd.DataFrame(topic_dist).reset_index(drop = True)\n",
    "\n",
    "# view the corresponding tf-idf dataframe with tf-idf values\n",
    "topic_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 topics. \n",
    "\n",
    "Topic 1 revolves around  support (words: thank, help, need, care). \n",
    "Topic 2 revolves around action (words: act, today, vote, protect, pass). \n",
    "Topic 3 revolves around safety (words: violence, assault, end gun violence, gun, violence, hope)\n",
    "Topic 4 revolves around democracy (words: impeachment, trump, president, election, democrats, voting, democracy, ballot, vote). \n",
    "Topic 5 means we need to clean and preprocess the data better to make better use of these functions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, let's analyze the sentiments contained within the tweets. You may use TextBlob or another library for these tasks. Do the following:\n",
    "\n",
    "* Choose two legislators, one who you think will be more liberal and one who you think will be more conservative, and analyze their sentiment and/or subjectivity scores per tweet. For instance, you might do two scatterplots that plot each legislator's sentiment against their subjectivity, or two density plots for their sentiments. Do the scores match what you thought?\n",
    "* Plot two more visualizations like the ones you chose in the first part, but do them to compare (1) Democrats v. Republicans and (2) Senators v. Representatives \n",
    "\n",
    "`TextBlob` has already been imported in the top cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature colum of sentiment polarity and subjectivity\n",
    "# ---------- \n",
    "\n",
    "# create new column feature of polarity\n",
    "subset_df['polarity'] = subset_df['tokens_full_text'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "\n",
    "# create new column feature of subjectivity\n",
    "subset_df['subjectivity'] = subset_df['tokens_full_text'].map(lambda text: TextBlob(text).sentiment.subjectivity)\n",
    "\n",
    "# view\n",
    "subset_df[['tokens_full_text', 'polarity']].head()\n",
    "\n",
    "# histogram of polarity\n",
    "sns.displot(subset_df, x=\"polarity\") \n",
    "plt.show()\n",
    "\n",
    "# plot\n",
    "sns.displot(subset_df, x=\"subjectivity\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop to view the 5 most positive tweets\n",
    "# ---------- \n",
    "for complaint in subset_df.nlargest(5, 'polarity')['text']:\n",
    "    print(complaint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop to view the 5 most positive tweets\n",
    "# ---------- \n",
    "for complaint in subset_df.nsmallest(5, 'polarity')['text']:\n",
    "    print(complaint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop to view the 5 most subjective tweets\n",
    "# ---------- \n",
    "for complaint in subset_df.nlargest(5, 'subjectivity')['text']:\n",
    "    print(complaint + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loop to view the 5 most subjective tweets\n",
    "# ---------- \n",
    "for complaint in subset_df.nsmallest(5, 'subjectivity')['text']:\n",
    "    print(complaint + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these measures look weird. Polarity is centered around 0 and subjectivity is centered around 0.5. However, we do find some difference when we do subgroup analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## analyzing Steve Chabot (conservative) and Barbara Lee (democrat)\n",
    "\n",
    "# Average polarity \n",
    "avg_polarity = subset_df['polarity'].mean()\n",
    "print(\"Average Polarity for Full Sample:\", avg_polarity)\n",
    "\n",
    "# Average polarity by democrats and republicans\n",
    "avg_polarity_by_party = subset_df.groupby('party')['polarity'].mean()\n",
    "avg_polarity_republicans = avg_polarity_by_party.get('Republican', 0)  # 0 is the default if 'Republican' is not found\n",
    "avg_polarity_democrats = avg_polarity_by_party.get('Democrat', 0)  # 0 is the default if 'Democratic' is not found\n",
    "print(\"Average Polarity for Republicans:\", avg_polarity_republicans)\n",
    "print(\"Average Polarity for Democrats:\", avg_polarity_democrats)\n",
    "\n",
    "# Average polarity by representatives and senators\n",
    "avg_polarity_by_position = subset_df.groupby('position')['polarity'].mean()\n",
    "avg_polarity_representatives = avg_polarity_by_position.get('Rep', 0)  \n",
    "avg_polarity_senators = avg_polarity_by_position.get('Sen', 0)  \n",
    "print(\"Average Polarity for Representatives:\", avg_polarity_representatives)\n",
    "print(\"Average Polarity for Senators:\", avg_polarity_senators)\n",
    "\n",
    "# Group by 'name_wikipedia' and calculate the mean polarity\n",
    "steve_chabot_avg_polarity = subset_df[subset_df['name_wikipedia'] == \"Steve Chabot\"]['polarity'].mean()\n",
    "print(\"Average Polarity for Steve Chabot:\", steve_chabot_avg_polarity)\n",
    "\n",
    "# Group by 'name_wikipedia' and calculate the mean polarity\n",
    "barbara_lee_avg_polarity = subset_df[subset_df['name_wikipedia'] == \"Barbara Lee\"]['polarity'].mean()\n",
    "print(\"Average Polarity for Barbara Lee:\", barbara_lee_avg_polarity)\n",
    "\n",
    "###### \n",
    "\n",
    "# Average subjectivity  \n",
    "avg_subjectivity = subset_df['subjectivity'].mean()\n",
    "print(\"Average Subjectivity for Full Sample:\", avg_subjectivity)\n",
    "\n",
    "# Average polarity by democrats and republicans\n",
    "avg_subjectivity_by_party = subset_df.groupby('party')['subjectivity'].mean()\n",
    "avg_subjectivity_republicans = avg_polarity_by_party.get('Republican', 0)  # 0 is the default if 'Republican' is not found\n",
    "avg_subjectivity_democrats = avg_polarity_by_party.get('Democrat', 0)  # 0 is the default if 'Democratic' is not found\n",
    "print(\"Average Subjectivity for Republicans:\", avg_subjectivity_republicans)\n",
    "print(\"Average Subjectivity for Democrats:\", avg_subjectivity_democrats)\n",
    "\n",
    "# Average subjetivity by representatives and senators\n",
    "avg_subjectivity_by_position = subset_df.groupby('position')['subjectivity'].mean()\n",
    "avg_subjectivity_representatives = avg_polarity_by_position.get('Rep', 0)  \n",
    "avg_subjectivity_senators = avg_polarity_by_position.get('Sen', 0)  \n",
    "print(\"Average Subjectivity for Representatives:\", avg_subjectivity_representatives)\n",
    "print(\"Average Subjectivity for Senators:\", avg_subjectivity_senators)\n",
    "\n",
    "# Group by 'name_wikipedia' and calculate the mean subjectivity \n",
    "steve_chabot_avg_subjectivity = subset_df[subset_df['name_wikipedia'] == \"Steve Chabot\"]['subjectivity'].mean()\n",
    "print(\"Average Subjectivity for Steve Chabot Waltz:\", steve_chabot_avg_subjectivity)\n",
    "\n",
    "# Group by 'name_wikipedia' and calculate the mean polarity\n",
    "barbara_lee_avg_subjectivity = subset_df[subset_df['name_wikipedia'] == \"Barbara Lee\"]['subjectivity'].mean()\n",
    "print(\"Average Subjectivity for Barbara Lee:\", barbara_lee_avg_subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a higher polarity score for Barbara Lee (liberal) relative to Steve Chabot (conservative), although it is pretty low for both of them so I don't know if I chose the best examples. Also, both of them are below the mean for the subjectivity index, and here again Barbara Lee (liberal) scores higher in subjectivity relative to Steve Chabot (conservative). I think I would have expected it to be the other way round. I also don't know how to interpret that 40% of the sample are being subjective, it seems a lot, but this is also tweets, right? It's the place for opinion, so perhaps it makes sense.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## scatter plot of polarity versus subjectivity for both legislators \n",
    "\n",
    "barbara_lee_data = subset_df[subset_df['name_wikipedia'] == 'Barbara Lee']\n",
    "steve_chabot_data = subset_df[subset_df['name_wikipedia'] == 'Steve Chabot']\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(barbara_lee_data['subjectivity'], barbara_lee_data['polarity'], label='Barbara Lee')\n",
    "plt.scatter(steve_chabot_data['subjectivity'], steve_chabot_data['polarity'], label='Steve Chabot')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Polarity')\n",
    "plt.title('Scatter Plot of Polarity vs Subjectivity for Barbara Lee (liberal) vs Steve Chabot (conservative)')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## scatter plot of sentiment versus polarity for democrats vs republicans \n",
    "\n",
    "democrat = subset_df[subset_df['party'] == 'Democrat']\n",
    "republican = subset_df[subset_df['party'] == 'Republican']\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(democrat['subjectivity'], democrat['polarity'], label='Democrat')\n",
    "plt.scatter(republican['subjectivity'], republican['polarity'], label='Republican')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Polarity')\n",
    "plt.title('Scatter Plot of Polarity vs Subjectivity for Democrats vs Republicans')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## scatter plot of sentiment versus polarity for representatives vs senators \n",
    "\n",
    "representative = subset_df[subset_df['position'] == 'Rep']\n",
    "senator = subset_df[subset_df['position'] == 'Sen']\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(representative['subjectivity'], representative['polarity'], label='Representative')\n",
    "plt.scatter(senator['subjectivity'], senator['polarity'], label='Senator')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Polarity')\n",
    "plt.title('Scatter Plot of Polarity vs Subjectivity for Representatives vs Senators')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first graph is not very conclusive to me, but maybe it's because by plotting only two legislators we don't have enough observations to see a pattern? \n",
    "However, if we plot polarity against subjectivity for all democrats and all republicans, we see that there's pretty much perfect overlap, so that relationship is going in the same direction for both groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot two more visualizations like the ones you chose in the first part, but do them to compare (1) Democrats v. Republicans and (2) Senators v. Representatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histograms that compare tweet length for republicans vs democrats and for senators vs representatives \n",
    "\n",
    "sns.displot(subset_df, \n",
    "            x=\"word_count\", \n",
    "            hue=\"party\", \n",
    "            col = \"party\") ## how do I tell it not to show independent without filtering the data before? \n",
    "plt.show()\n",
    "\n",
    "sns.displot(subset_df, \n",
    "            x=\"word_count\", \n",
    "            hue=\"position\", \n",
    "           col = \"position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tweets coming from Democrats relative to Republicans, but the distribution and average length are similar. \n",
    "There are more tweets coming from Representatives relative to Senators, but the distributions are also not that different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to classification, explore different featurization techniques. Create three dataframes or arrays to represent your text features, specifically:\n",
    "\n",
    "* Features engineered from your previous analysis. For example, word counts, sentiment scores, topic model etc.\n",
    "* A term frequency-inverse document frequency matrix. \n",
    "* An embedding-based featurization (like a document averaged word2vec)\n",
    "\n",
    "In the next section, you will experiment with each of these featurization techniques to see which one produces the best classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineered Text Features: select engineered text features\n",
    "engineered_features = subset_df[['tweet_length', \n",
    "                                 'word_count', \n",
    "                                 'polarity', \n",
    "                                 'subjectivity']].reset_index(drop = True)\n",
    "\n",
    "engineered_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Model + Engineered Features: combine results from our topic model + engineered features\n",
    "engineered_features_with_topics = topic_dist_df.join(engineered_features.reset_index(drop = True))\n",
    "\n",
    "engineered_features_with_topics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words or Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of tf-idf \n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.todense(),                # specify matrix\n",
    "                        columns = tf.get_feature_names_out())  # set feature names\n",
    "\n",
    "tfidf_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Word2Vec model from Google; OPTIONAL depending on your computational resources (the file is ~1 GB)\n",
    "# Also note that this file path assumes that the word vectors are underneath 'data'; you may wish to point to the CSS course repo and change the path\n",
    "# or move the vector file to the project repo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model_path = 'data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = word2vec_model['word']\n",
    "\n",
    "model = gensim.models.Word2Vec(subset_df['tokens'],    # specify data - sentences\n",
    "                               vector_size=100,        # set embedding size at 100\n",
    "                               window=5,               # max distance between current and predicted word\n",
    "                               min_count=5,            # ignores words with freq fewer than this threshold\n",
    "                               sg=0,                   # specify Continuous Bag of Words Algorithim\n",
    "                               alpha=0.025,            # learning rate\n",
    "                               epochs = 5,             # iterations\n",
    "                               seed = 12,              # set random seed (same as random_state in sklearn )\n",
    "                               batch_words=10000,      # sample size \n",
    "                               workers = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create function to iterate over every token and document in our corpus\n",
    "# ---------\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # Filter out words that are not present in the vocabulary\n",
    "    doc = [word for word in doc if word in word2vec_model.key_to_index]\n",
    "    if doc:  # Check if doc is not empty\n",
    "        return np.mean(word2vec_model.__getitem__(doc), axis=0)\n",
    "    else:\n",
    "        # Return None or any default value if doc is empty\n",
    "        return np.zeros(word2vec_model.vector_size)  # Return zero vector if doc is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create an array for the size of the corpus\n",
    "# ----------\n",
    "# create empty list\n",
    "empty_list_embeddings_means = []\n",
    "\n",
    "# loop over each each token\n",
    "for doc in subset_df['tokens']: # append the vector for each document\n",
    "    empty_list_embeddings_means.append(document_vector(word2vec_model, doc))\n",
    "\n",
    "# convert the list to array\n",
    "doc_average_embeddings = np.array(empty_list_embeddings_means) \n",
    "\n",
    "# make df of averages\n",
    "word2vec_df = pd.DataFrame(doc_average_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return a list of word for which we have calculations\n",
    "# ----------\n",
    "words = list(model.wv.index_to_key)\n",
    "print(words[0:100])  # print the first 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## just for fun \n",
    "\n",
    "# similarity between Democrat and Republican\n",
    "# ----------\n",
    "similarity_democrat_republican = model.wv.similarity('Democrat', 'Republican')\n",
    "print(\"Similarity between democrat and republican:\", similarity_democrat_republican)\n",
    "\n",
    "# similarty between act and speak\n",
    "# ----------\n",
    "similarity_act_speak = model.wv.similarity('act', 'speak')\n",
    "print(\"Similarity between act and speak:\", similarity_act_speak)\n",
    "\n",
    "# which doesn't belong? 1\n",
    "# ----------\n",
    "which_does_not_belong = model.wv.doesnt_match(['Democrat', 'Republican', 'Americans']) ## so interesting that it's picking Democrat here as the word that does NOT belong \n",
    "print(\"Which does not belong between Democrat, Republcian and Americans?:\", which_does_not_belong)\n",
    "\n",
    "# vector math\n",
    "# ----------\n",
    "most_similar = model.wv.most_similar(positive=['Democrat', 'act', 'today'], \n",
    "                      negative=['impeachment', 'law'])\n",
    "print(\"Which is most similar?:\", most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use cross-validation or partition your data with training/validation/test sets for this section. Do the following:\n",
    "\n",
    "* Choose a supervised learning algorithm such as logistic regression, random forest etc. \n",
    "* Train six models. For each of the three dataframes you created in the featurization part, train one model to predict whether the author of the tweet is a Democrat or Republican, and a second model to predict whether the author is a Senator or Representative.\n",
    "* Report the accuracy and other relevant metrics for each of these six models.\n",
    "* Choose the featurization technique associated with your best model. Combine those text features with non-text features. Train two more models: (1) A supervised learning algorithm that uses just the non-text features and (2) a supervised learning algorithm that combines text and non-text features. Report accuracy and other relevant metrics. \n",
    "\n",
    "If time permits, you are encouraged to use hyperparameter tuning or AutoML techniques like TPOT, but are not explicitly required to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Six Models with Just Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# six models ([engineered features, frequency-based, embedding] * [democrat/republican, senator/representative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = LogisticRegression(max_iter=500)\n",
    "# df = dataframes[0]\n",
    "# model.fit(df.to_numpy(), labels[0][0].argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a list of dataframes from the objects created above\n",
    "dataframes = [engineered_features, tfidf_df]\n",
    "dataframe_names = ['engineered_features', 'tfidf_df']\n",
    "\n",
    "# Create label binarizer for party (Democrat/Republican)\n",
    "party_lb = LabelBinarizer()\n",
    "party_labels = party_lb.fit_transform(subset_df[\"party\"])  # Binarize party labels\n",
    "\n",
    "# Create label binarizer for party (Democrat/Republican)\n",
    "position_lb = LabelBinarizer()\n",
    "position_labels = position_lb.fit_transform(subset_df[\"position\"])  # Binarize position labels\n",
    "\n",
    "labels = [(party_labels, 'Party Affiliation'), (position_labels, 'Position')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## I cannot get the function to work so I'm doing one by one. Sorry about that!     \n",
    "\n",
    "# engineered_features - party \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(engineered_features, \n",
    "                                                    party_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# engineered_features - position \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(engineered_features, \n",
    "                                                \n",
    "                                                    position_labels, \n",
    "                                                \n",
    "                                                    test_size=0.2, \n",
    "                                                \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## tfidf_df - party\n",
    "    \n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, \n",
    "                                                    party_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## tfidf_df - position\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_df, \n",
    "                                                    position_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## word_embeddings - party\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_average_embeddings, \n",
    "                                                    party_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## word_embeddings - position\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(doc_average_embeddings, \n",
    "                                                    position_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Combined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two models ([best text features + non-text features] * [democrat/republican, senator/representative])\n",
    "\n",
    "## tf-idf worked better then engineered features, so I'm comibing this df with text features and non text features. \n",
    "\n",
    "# Non-text features\n",
    "# --------\n",
    "# select non-text features\n",
    "non_text_features = subset_df[['state', \n",
    "                               'position', \n",
    "                               'gender',\n",
    "                               'birthday', \n",
    "                               'joined_congress_date', \n",
    "                               'clinton_2016_state_share',\n",
    "                               'obama_2012_state_share', \n",
    "                               'romney_2012_state_share']]\n",
    "\n",
    "# get dummies of non-text features\n",
    "non_text_features_dummies = pd.get_dummies(non_text_features).reset_index(drop = True)\n",
    "\n",
    "# combine non-text features with tf-idf  \n",
    "non_text_plus_tfidf = non_text_features_dummies.reset_index(drop = True).join(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## non-text features - party\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(non_text_features_dummies, \n",
    "                                                    party_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## tfidf_df non-text features - party\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(non_text_plus_tfidf, \n",
    "                                                    party_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## non-text features - position\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(non_text_features_dummies, \n",
    "                                                    position_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## tfidf_df non-text features - position\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(non_text_plus_tfidf, \n",
    "                                                    position_labels, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train,    \n",
    "            y_train.ravel())\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{confusion_mat}')\n",
    "print(f'Classification Report:\\n{class_report}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do standard preprocessing techniques need to be further customized to a particular corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard preprocessing techniques provide a good starting point for text analysis. However, it's necessary to adapt these techniques to the specific needs and context of your dataset to be able to perform a more accurate analysis. Some examples might be the following: \n",
    "\n",
    "** Domain-specific terminology or jargon, such as abbreviations or acronyms.\n",
    "\n",
    "** Irregularities, such as misspellings, grammatical errors, or slang, or the use of informal language. \n",
    "\n",
    "** Sentiment analysis may require handling negation, sarcasm, formality or emoticons, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you find evidence for the idea that Democrats and Republicans have different sentiments in their tweets? What about Senators and Representatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can see that republicans have, on average, a higher polarity score (0.14 republicans vs 0.10 democrats) and higher subjectivity score (again, 0.14 republicans vs. 0.10 democrats). In the particular example given the legislators that I randomly picked, I found it was the other way round, meaning that the democratic legislator had both a higher polarity and subjectivity score, but perhaps I did not pick good examples that extrapolate over the overall sample. We do not see, however, the same trend for representatives and senators. Both representatives and senators show a polarity and subjectivity score of 0.11. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is validating your exploratory and unsupervised learning approaches with a supervised learning algorithm valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating exploratory and unsupervised learning approaches with a supervised learning algorithm can be valuable for the following reasons: \n",
    "\n",
    "** Because supervised learning algorithms are trained on labeled data, they allow for a more objective assessment of the effectiveness and accuracy of the unsupervised methods.\n",
    "\n",
    "** Exploratory and unsupervised learning methods help reveal hidden patterns in data. When we confirm these findings using supervised learning algorithms, we might get better insights or it might give us a better understanding of the data's underlying structure.\n",
    "\n",
    "** Validating unsupervised techniques with supervised models helps achieve generalizability of the insights and patterns to be applied to unseen data. \n",
    "\n",
    "** Validating unsupervised techniques with supervised models can help in explaining relationships between features and target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Did text only, non-text only, or text and non-text features together perform the best? What is the intuition behind combining text and non-text features in a supervised learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text only has a lower accuracy score (73% for party and 77% for position), but either using non-text features or using text and non-text features combined has an accuracy score of 98% or higher. Intuition: Combining text and non-text features in a supervised learning algorithm allows us to take advantage of the strengths of both types of features. Text features capture the linguistic content and semantics of the data, while non-text features provide additional context or domain knowledge that we cannot get from text alone. By combining both, we can improve predictive performance. What is not so clear to me here (maybe I made a coding mistake) is why non-text features alone was better than text alone. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
