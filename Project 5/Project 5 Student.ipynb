{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Computational Social Science] Project 5: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will use natural language processing techniques to explore a dataset containing tweets from members of the 116th United States Congress that met from January 3, 2019 to January 2, 2021. The dataset has also been cleaned to contain information about each legislator. Concretely, you will do the following:\n",
    "\n",
    "* Preprocess the text of legislators' tweets\n",
    "* Conduct Exploratory Data Analysis of the text\n",
    "* Use sentiment analysis to explore differences between legislators' tweets\n",
    "* Featurize text with manual feature engineering, frequency-based, and vector-based techniques\n",
    "* Predict legislators' political parties and whether they are a Senator or Representative\n",
    "\n",
    "You will explore two questions that relate to two central findings in political science and examine how they relate to the text of legislators' tweets. First, political scientists have argued that U.S. politics is currently highly polarized relative to other periods in American history, but also that the polarization is asymmetric. Historically, there were several conservative Democrats (i.e. \"blue dog Democrats\") and liberal Republicans (i.e. \"Rockefeller Republicans\"), as measured by popular measurement tools like [DW-NOMINATE](https://en.wikipedia.org/wiki/NOMINATE_(scaling_method)#:~:text=DW\\%2DNOMINATE\\%20scores\\%20have\\%20been,in\\%20the\\%20liberal\\%2Dconservative\\%20scale.). However, in the last few years, there are few if any examples of any Democrat in Congress being further to the right than any Republican and vice versa. At the same time, scholars have argued that this polarization is mostly a function of the Republican party moving further right than the Democratic party has moved left. **Does this sort of asymmetric polarization show up in how politicians communicate to their constituents through tweets?**\n",
    "\n",
    "Second, the U.S. Congress is a bicameral legislature, and there has long been debate about partisanship in the Senate versus the House. The House of Representatives is apportioned by population and all members serve two year terms. In the Senate, each state receives two Senators and each Senator serves a term of six years. For a variety of reasons (smaller chamber size, more insulation from the voters, rules and norms like the filibuster, etc.), the Senate has been argued to be the \"cooling saucer\" of Congress in that it is more bipartisan and moderate than the House. **Does the theory that the Senate is more moderate have support in Senators' tweets?**\n",
    "\n",
    "**Note**: See the project handout for more details on caveats and the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/69/67/7116bafd0aa853f4101a282defcb9d6dd8a32a76b5c4f8edf7603cbf8224/spacy-3.7.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading spacy-3.7.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/05/29/5f48eea8712697f66531c4b6018b1713a3aec2b4eddbce1c63f93adbf6b1/murmurhash-1.0.10-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/20/1f/2ae07056430a0276e0cbd765652db82ea153c5fb2a3d753fbffd553827d5/cymem-2.0.8-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading cymem-2.0.8-cp311-cp311-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/c0/1e/05fa559f53b635d96b233b63e93accb75215025b997486f7290991bec6c3/preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n",
      "  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/4a/88/344dd58afa62deecae6743efb38b56f7f9eacdad1e7a5cab05fffa8cf6b7/thinc-8.2.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading thinc-8.2.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/40/fe/baa4056b7e8585f4c3478d3d1d3a2c1c3095ff066e4fb420bb000abb6cc2/srsly-2.4.8-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading srsly-2.4.8-cp311-cp311-macosx_10_9_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 from https://files.pythonhosted.org/packages/e4/37/3ffe6e7daa1ea1b4bf5228807a92ccbae538cf57c0c50b93564c310c11a8/pydantic-2.6.0-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.6.0-py3-none-any.whl.metadata (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.3)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Obtaining dependency information for pydantic-core==2.16.1 from https://files.pythonhosted.org/packages/25/95/1357b15051f458a15eeaf03d35d0f7466ec8979eb69061dacc6d8f7924d9/pydantic_core-2.16.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.16.1-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/c7/59/c8010f380a16709e6d3ef5534845d1ca1e689079914ec67ab60f57edfc37/blis-0.7.11-cp311-cp311-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading blis-0.7.11-cp311-cp311-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.7.2-cp311-cp311-macosx_10_9_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-macosx_10_9_x86_64.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-macosx_10_9_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-macosx_10_9_x86_64.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.0/133.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.0-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.1-cp311-cp311-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-macosx_10_9_x86_64.whl (490 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.0/490.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.2-cp311-cp311-macosx_10_9_x86_64.whl (861 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m861.5/861.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp311-cp311-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.0 pydantic-core-2.16.1 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.0)\n",
      "Requirement already satisfied: jinja2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/brenda/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download libraries\n",
    "# ----------\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as numpy\n",
    "\n",
    "# punctuation, stop words and English language model\n",
    "from string import punctuation\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# countvectorizer, tfidfvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>name_wikipedia</th>\n",
       "      <th>position</th>\n",
       "      <th>joined_congress_date</th>\n",
       "      <th>birthday</th>\n",
       "      <th>gender</th>\n",
       "      <th>state</th>\n",
       "      <th>district_number</th>\n",
       "      <th>party</th>\n",
       "      <th>trump_2016_state_share</th>\n",
       "      <th>clinton_2016_state_share</th>\n",
       "      <th>obama_2012_state_share</th>\n",
       "      <th>romney_2012_state_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.081010e+18</td>\n",
       "      <td>RepByrne</td>\n",
       "      <td>2019-01-03T21:23:00-05:00</td>\n",
       "      <td>Great news for Baldwin County! The economy of ...</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>Rep</td>\n",
       "      <td>8-Jan-14</td>\n",
       "      <td>2/16/1955</td>\n",
       "      <td>M</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1,318,255</td>\n",
       "      <td>729,547</td>\n",
       "      <td>795,696</td>\n",
       "      <td>1,255,925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.080880e+18</td>\n",
       "      <td>RepByrne</td>\n",
       "      <td>2019-01-03T12:30:38-05:00</td>\n",
       "      <td>Outstanding news today from @Airbus! @JetBlue ...</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>Rep</td>\n",
       "      <td>8-Jan-14</td>\n",
       "      <td>2/16/1955</td>\n",
       "      <td>M</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1,318,255</td>\n",
       "      <td>729,547</td>\n",
       "      <td>795,696</td>\n",
       "      <td>1,255,925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.080830e+18</td>\n",
       "      <td>RepByrne</td>\n",
       "      <td>2019-01-03T09:12:07-05:00</td>\n",
       "      <td>RT @senatemajldr Democrats will have to get se...</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>Rep</td>\n",
       "      <td>8-Jan-14</td>\n",
       "      <td>2/16/1955</td>\n",
       "      <td>M</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1,318,255</td>\n",
       "      <td>729,547</td>\n",
       "      <td>795,696</td>\n",
       "      <td>1,255,925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.080890e+18</td>\n",
       "      <td>RepByrne</td>\n",
       "      <td>2019-01-03T13:20:53-05:00</td>\n",
       "      <td>Here is a sign of things to come: As Democrats...</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>Rep</td>\n",
       "      <td>8-Jan-14</td>\n",
       "      <td>2/16/1955</td>\n",
       "      <td>M</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1,318,255</td>\n",
       "      <td>729,547</td>\n",
       "      <td>795,696</td>\n",
       "      <td>1,255,925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.080870e+18</td>\n",
       "      <td>RepByrne</td>\n",
       "      <td>2019-01-03T12:10:26-05:00</td>\n",
       "      <td>Let's understand what we're dealing with here:...</td>\n",
       "      <td>Bradley Byrne</td>\n",
       "      <td>Rep</td>\n",
       "      <td>8-Jan-14</td>\n",
       "      <td>2/16/1955</td>\n",
       "      <td>M</td>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "      <td>Republican</td>\n",
       "      <td>1,318,255</td>\n",
       "      <td>729,547</td>\n",
       "      <td>795,696</td>\n",
       "      <td>1,255,925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id screen_name                   datetime  \\\n",
       "0  1.081010e+18    RepByrne  2019-01-03T21:23:00-05:00   \n",
       "1  1.080880e+18    RepByrne  2019-01-03T12:30:38-05:00   \n",
       "2  1.080830e+18    RepByrne  2019-01-03T09:12:07-05:00   \n",
       "3  1.080890e+18    RepByrne  2019-01-03T13:20:53-05:00   \n",
       "4  1.080870e+18    RepByrne  2019-01-03T12:10:26-05:00   \n",
       "\n",
       "                                                text name_wikipedia position  \\\n",
       "0  Great news for Baldwin County! The economy of ...  Bradley Byrne      Rep   \n",
       "1  Outstanding news today from @Airbus! @JetBlue ...  Bradley Byrne      Rep   \n",
       "2  RT @senatemajldr Democrats will have to get se...  Bradley Byrne      Rep   \n",
       "3  Here is a sign of things to come: As Democrats...  Bradley Byrne      Rep   \n",
       "4  Let's understand what we're dealing with here:...  Bradley Byrne      Rep   \n",
       "\n",
       "  joined_congress_date   birthday gender state district_number       party  \\\n",
       "0             8-Jan-14  2/16/1955      M    AL               1  Republican   \n",
       "1             8-Jan-14  2/16/1955      M    AL               1  Republican   \n",
       "2             8-Jan-14  2/16/1955      M    AL               1  Republican   \n",
       "3             8-Jan-14  2/16/1955      M    AL               1  Republican   \n",
       "4             8-Jan-14  2/16/1955      M    AL               1  Republican   \n",
       "\n",
       "  trump_2016_state_share clinton_2016_state_share obama_2012_state_share  \\\n",
       "0              1,318,255                  729,547                795,696   \n",
       "1              1,318,255                  729,547                795,696   \n",
       "2              1,318,255                  729,547                795,696   \n",
       "3              1,318,255                  729,547                795,696   \n",
       "4              1,318,255                  729,547                795,696   \n",
       "\n",
       "  romney_2012_state_share  \n",
       "0               1,255,925  \n",
       "1               1,255,925  \n",
       "2               1,255,925  \n",
       "3               1,255,925  \n",
       "4               1,255,925  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "# ----------\n",
    "congress_tweets = pd.read_csv(\"data/116th Congressional Tweets and Demographics.csv\")\n",
    "congress_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in working with text data is to preprocess it. Make sure you do the following:\n",
    "\n",
    "* Remove punctuation and stop words. The `rem_punc_stop()` function we used in lab is provided to you but you should feel free to edit it as necessary for other steps\n",
    "* Remove tokens that occur frequently in tweets, but may not be helpful for downstream classification. For instance, many tweets contain a flag for retweeting, or share a URL \n",
    "\n",
    "As you search online, you might run into solutions that rely on regular expressions. You are free to use these, but you should also be able to preprocess using the techniques we covered in lab. Specifically, we encourage you to use spaCy's token attributes and string methods to do some of this text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load STOP_WORDS module from spaCy library\n",
    "# ----------\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load punctuation module from string library\n",
    "# ----------\n",
    "from string import punctuation  # note that this is a base library\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## defining function to remove punctuation, stop words and other pre-defined characters \n",
    "def rem_punc_stop(text):\n",
    "    ## tokenize\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    ## extract tokens (not including tokens if they are stop words, spaces or punctuation, and I'm also removing any alpha numeric character)\n",
    "    spacy_words = [token.text for token in doc if not token.is_stop and not token.is_space and not token.is_punct and (token.is_alpha or token.is_digit)]\n",
    "    \n",
    "    ## removing URL \n",
    "    spacy_words = [word for word in spacy_words if not word.startswith('http')]\n",
    "    \n",
    "    # remove flags for retweeting (e.g., RT)\n",
    "    spacy_words = [word for word in spacy_words if word.lower() != 'rt']\n",
    "     \n",
    "    return spacy_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## keeping just the column I'm interested in and a subset up to row 20,000. My computer freezes if I use the full sample, but I'll try to do run this for the full sample before doing the analysis for the next problem set. \n",
    "congress_tweets_text = congress_tweets.loc[:20000, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## creating a data set with the same number of rows that I'm using to clean the text column \n",
    "congress_tweets_small = congress_tweets.loc[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## applying rem_punc_stop function to this column \n",
    "congress_tweets_tokens = congress_tweets_text.apply(rem_punc_stop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amp                5453\n",
       "QT                 2442\n",
       "Alabama            2119\n",
       "today              1939\n",
       "American           1512\n",
       "border             1441\n",
       "House              1440\n",
       "Today              1306\n",
       "work               1289\n",
       "Democrats          1273\n",
       "time               1263\n",
       "Thank              1248\n",
       "Congress           1194\n",
       "people             1172\n",
       "Arizona            1138\n",
       "Trump              1081\n",
       "President          1046\n",
       "help               1043\n",
       "America            1008\n",
       "great               990\n",
       "Americans           970\n",
       "support             963\n",
       "bill                956\n",
       "state               937\n",
       "Act                 931\n",
       "week                873\n",
       "country             863\n",
       "new                 840\n",
       "need                839\n",
       "nation              803\n",
       "years               793\n",
       "working             790\n",
       "day                 726\n",
       "year                689\n",
       "continue            680\n",
       "security            673\n",
       "office              670\n",
       "Thanks              663\n",
       "vote                656\n",
       "Senate              647\n",
       "care                644\n",
       "crisis              643\n",
       "women               633\n",
       "health              625\n",
       "national            618\n",
       "federal             614\n",
       "Great               608\n",
       "families            607\n",
       "proud               592\n",
       "service             592\n",
       "important           588\n",
       "news                578\n",
       "communities         575\n",
       "community           574\n",
       "like                571\n",
       "forward             570\n",
       "law                 569\n",
       "protect             563\n",
       "honor               560\n",
       "right               551\n",
       "morning             542\n",
       "jobs                542\n",
       "AZ                  539\n",
       "family              538\n",
       "members             526\n",
       "AL                  516\n",
       "know                511\n",
       "veterans            500\n",
       "join                500\n",
       "economy             499\n",
       "talk                495\n",
       "fight               489\n",
       "students            486\n",
       "life                486\n",
       "funding             486\n",
       "Read                484\n",
       "Alaska              482\n",
       "lives               471\n",
       "hearing             471\n",
       "businesses          460\n",
       "County              452\n",
       "1                   447\n",
       "National            440\n",
       "legislation         440\n",
       "local               440\n",
       "public              438\n",
       "home                436\n",
       "safe                434\n",
       "government          430\n",
       "illegal             426\n",
       "Washington          425\n",
       "colleagues          424\n",
       "Watch               420\n",
       "million             420\n",
       "military            414\n",
       "workers             410\n",
       "discuss             409\n",
       "impeachment         408\n",
       "including           392\n",
       "way                 391\n",
       "ensure              386\n",
       "visit               382\n",
       "future              377\n",
       "efforts             376\n",
       "want                373\n",
       "issues              373\n",
       "good                371\n",
       "Day                 366\n",
       "stop                365\n",
       "Happy               364\n",
       "passed              363\n",
       "Dems                360\n",
       "live                360\n",
       "serve               357\n",
       "world               357\n",
       "sure                354\n",
       "small               353\n",
       "secure              353\n",
       "2                   352\n",
       "economic            350\n",
       "stand               349\n",
       "provide             348\n",
       "better              348\n",
       "look                344\n",
       "Congratulations     343\n",
       "meeting             341\n",
       "access              340\n",
       "Congressional       338\n",
       "Pelosi              335\n",
       "bipartisan          334\n",
       "2020                330\n",
       "history             329\n",
       "Committee           322\n",
       "District            322\n",
       "critical            320\n",
       "opportunity         320\n",
       "leaders             318\n",
       "team                311\n",
       "2019                306\n",
       "State               306\n",
       "resources           304\n",
       "best                303\n",
       "program             300\n",
       "Mobile              300\n",
       "job                 298\n",
       "information         297\n",
       "open                297\n",
       "leadership          296\n",
       "United              294\n",
       "end                 293\n",
       "alpolitics          288\n",
       "come                287\n",
       "hope                284\n",
       "Mo                  283\n",
       "joining             283\n",
       "power               282\n",
       "joined              281\n",
       "men                 280\n",
       "election            279\n",
       "States              277\n",
       "Socialist           275\n",
       "pm                  274\n",
       "ago                 270\n",
       "Capitol             269\n",
       "high                269\n",
       "Republicans         268\n",
       "children            267\n",
       "pandemic            267\n",
       "strong              265\n",
       "long                264\n",
       "report              264\n",
       "rural               263\n",
       "meet                261\n",
       "wall                261\n",
       "hear                261\n",
       "rights              260\n",
       "emergency           259\n",
       "staff               259\n",
       "deserve             259\n",
       "hard                259\n",
       "thank               257\n",
       "address             253\n",
       "remember            252\n",
       "southern            252\n",
       "Proud               249\n",
       "taking              247\n",
       "3                   247\n",
       "making              247\n",
       "fighting            247\n",
       "action              247\n",
       "coronavirus         247\n",
       "having              247\n",
       "tonight             245\n",
       "Let                 245\n",
       "pass                243\n",
       "voted               243\n",
       "business            243\n",
       "Border              241\n",
       "system              240\n",
       "Mueller             240\n",
       "political           239\n",
       "free                238\n",
       "let                 237\n",
       "ICYMI               236\n",
       "Arizonans           236\n",
       "lost                236\n",
       "introduced          235\n",
       "friends             235\n",
       "place               234\n",
       "coming              233\n",
       "needs               233\n",
       "process             231\n",
       "said                231\n",
       "questions           229\n",
       "receive             229\n",
       "safety              229\n",
       "going               228\n",
       "states              228\n",
       "DC                  227\n",
       "night               227\n",
       "SOTU                227\n",
       "John                227\n",
       "freedom             226\n",
       "voting              226\n",
       "grateful            226\n",
       "town                225\n",
       "Brooks              224\n",
       "Huntsville          224\n",
       "celebrate           224\n",
       "pay                 223\n",
       "immigration         222\n",
       "issue               222\n",
       "yesterday           222\n",
       "Alabamians          221\n",
       "officials           218\n",
       "school              217\n",
       "enforcement         217\n",
       "continues           215\n",
       "floor               213\n",
       "use                 212\n",
       "Democrat            212\n",
       "China               211\n",
       "past                209\n",
       "10                  209\n",
       "step                207\n",
       "Tune                207\n",
       "learn               206\n",
       "Security            205\n",
       "infrastructure      204\n",
       "district            203\n",
       "letter              203\n",
       "stay                201\n",
       "Valley              201\n",
       "friend              201\n",
       "Montgomery          201\n",
       "statement           199\n",
       "violence            199\n",
       "New                 197\n",
       "education           196\n",
       "order               196\n",
       "plan                195\n",
       "days                194\n",
       "water               194\n",
       "month               193\n",
       "energy              193\n",
       "administration      192\n",
       "tomorrow            192\n",
       "Congressman         191\n",
       "away                190\n",
       "honored             190\n",
       "bring               190\n",
       "Veterans            190\n",
       "Alaskans            188\n",
       "assistance          188\n",
       "role                188\n",
       "Center              188\n",
       "4                   187\n",
       "space               185\n",
       "Coronavirus         185\n",
       "Secretary           185\n",
       "weekend             185\n",
       "says                184\n",
       "opportunities       183\n",
       "impact              183\n",
       "event               182\n",
       "find                182\n",
       "times               181\n",
       "citizens            181\n",
       "helping             181\n",
       "VA                  181\n",
       "clear               180\n",
       "billion             179\n",
       "act                 179\n",
       "justice             179\n",
       "money               179\n",
       "face                178\n",
       "member              177\n",
       "5                   177\n",
       "Southern            176\n",
       "allow               176\n",
       "signed              175\n",
       "City                175\n",
       "special             175\n",
       "especially          175\n",
       "number              174\n",
       "growth              174\n",
       "debt                174\n",
       "story               174\n",
       "Administration      173\n",
       "hall                173\n",
       "protecting          173\n",
       "line                172\n",
       "spending            172\n",
       "latest              171\n",
       "relief              171\n",
       "trade               171\n",
       "EST                 171\n",
       "100                 170\n",
       "resolution          170\n",
       "available           170\n",
       "change              170\n",
       "policies            170\n",
       "benefits            170\n",
       "thanks              169\n",
       "response            169\n",
       "able                169\n",
       "shutdown            169\n",
       "aliens              168\n",
       "increase            168\n",
       "6                   168\n",
       "School              167\n",
       "possible            167\n",
       "real                167\n",
       "getting             167\n",
       "area                166\n",
       "Check               166\n",
       "tax                 166\n",
       "healthcare          166\n",
       "prayers             165\n",
       "hold                164\n",
       "Speaker             164\n",
       "Department          164\n",
       "discussion          164\n",
       "brave               163\n",
       "left                163\n",
       "Court               163\n",
       "Phoenix             162\n",
       "Enjoyed             162\n",
       "funds               162\n",
       "defense             162\n",
       "research            161\n",
       "Rights              161\n",
       "group               161\n",
       "met                 160\n",
       "industry            160\n",
       "agents              159\n",
       "decision            159\n",
       "far                 158\n",
       "Looking             158\n",
       "supporting          157\n",
       "glad                157\n",
       "constituents        157\n",
       "police              156\n",
       "tour                156\n",
       "Chairman            156\n",
       "committed           156\n",
       "young               156\n",
       "appreciate          155\n",
       "medical             155\n",
       "afternoon           155\n",
       "officers            154\n",
       "bills               154\n",
       "announced           154\n",
       "development         154\n",
       "sacrifice           154\n",
       "hours               154\n",
       "legacy              153\n",
       "threat              153\n",
       "Learn               153\n",
       "8                   153\n",
       "policy              153\n",
       "11                  152\n",
       "human               152\n",
       "Glad                151\n",
       "Mexico              151\n",
       "soon                151\n",
       "CT                  151\n",
       "president           151\n",
       "pro                 150\n",
       "Program             149\n",
       "Good                149\n",
       "took                149\n",
       "improve             148\n",
       "served              148\n",
       "hosting             148\n",
       "continued           148\n",
       "expand              148\n",
       "Constitution        148\n",
       "encourage           148\n",
       "WATCH               148\n",
       "Caucus              147\n",
       "gun                 147\n",
       "update              147\n",
       "record              146\n",
       "Census              146\n",
       "big                 146\n",
       "Republican          146\n",
       "serving             146\n",
       "effort              145\n",
       "build               144\n",
       "providing           144\n",
       "thousands           144\n",
       "love                144\n",
       "rate                143\n",
       "Congrats            143\n",
       "Tucson              143\n",
       "share               143\n",
       "historic            143\n",
       "farmers             143\n",
       "Protection          142\n",
       "unemployment        142\n",
       "humanitarian        141\n",
       "holding             141\n",
       "General             141\n",
       "lead                141\n",
       "incredible          140\n",
       "East                140\n",
       "needed              140\n",
       "recently            140\n",
       "watch               139\n",
       "win                 139\n",
       "create              139\n",
       "apply               139\n",
       "importance          138\n",
       "partisan            138\n",
       "Mayor               137\n",
       "Join                137\n",
       "talking             137\n",
       "laws                137\n",
       "weather             137\n",
       "aid                 137\n",
       "called              136\n",
       "USA                 136\n",
       "risk                135\n",
       "save                135\n",
       "wo                  134\n",
       "birthday            134\n",
       "Health              133\n",
       "commitment          133\n",
       "remain              133\n",
       "visiting            133\n",
       "March               132\n",
       "votes               132\n",
       "anniversary         132\n",
       "months              132\n",
       "believe             132\n",
       "hospitals           132\n",
       "priorities          132\n",
       "politics            132\n",
       "speech              132\n",
       "employees           131\n",
       "USMCA               130\n",
       "foreign             130\n",
       "online              130\n",
       "ways                130\n",
       "elections           130\n",
       "Association         130\n",
       "speak               130\n",
       "dangerous           129\n",
       "conversation        129\n",
       "combat              129\n",
       "Service             129\n",
       "Gulf                129\n",
       "heroes              129\n",
       "Birmingham          128\n",
       "elected             128\n",
       "ones                128\n",
       "ready               128\n",
       "prevent             127\n",
       "excited             127\n",
       "looking             127\n",
       "major               127\n",
       "loved               127\n",
       "Selma               126\n",
       "complete            126\n",
       "Friday              126\n",
       "Members             126\n",
       "leading             125\n",
       "rule                125\n",
       "victims             125\n",
       "COMING              125\n",
       "Sally               124\n",
       "agree               124\n",
       "BREAKING            124\n",
       "folks               124\n",
       "millions            124\n",
       "represent           124\n",
       "trying              124\n",
       "abortion            124\n",
       "nearly              124\n",
       "dtype: int64"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## seeing the frequency with which tokens appear to see if it makes sense to remove any more tokens. \n",
    "tokens_all_tweets = sum(congress_tweets_tokens, [])\n",
    "pd.Series(tokens_all_tweets).value_counts().sort_values(ascending=False).head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## joining the new cleaned column with the subsetted dataset \n",
    "congress_tweets_small.loc['tokens'] = congress_tweets_tokens.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use two of the techniques we covered in lab (or other techniques outside of lab!) to explore the text of the tweets. You should construct these visualizations with an eye toward the eventual classification tasks: (1) predicting the legislator's political party based on the text of their tweet, and (2) predicting whether the legislator is a Senator or Representative. As a reminder, in lab we covered word frequencies, word clouds, word/character counts, scattertext, and topic modeling as possible exploration tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's analyze the sentiments contained within the tweets. You may use TextBlob or another library for these tasks. Do the following:\n",
    "\n",
    "* Choose two legislators, one who you think will be more liberal and one who you think will be more conservative, and analyze their sentiment and/or subjectivity scores per tweet. For instance, you might do two scatterplots that plot each legislator's sentiment against their subjectivity, or two density plots for their sentiments. Do the scores match what you thought?\n",
    "* Plot two more visualizations like the ones you chose in the first part, but do them to compare (1) Democrats v. Republicans and (2) Senators v. Representatives \n",
    "\n",
    "`TextBlob` has already been imported in the top cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going to classification, explore different featurization techniques. Create three dataframes or arrays to represent your text features, specifically:\n",
    "\n",
    "* Features engineered from your previous analysis. For example, word counts, sentiment scores, topic model etc.\n",
    "* A term frequency-inverse document frequency matrix. \n",
    "* An embedding-based featurization (like a document averaged word2vec)\n",
    "\n",
    "In the next section, you will experiment with each of these featurization techniques to see which one produces the best classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineered Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineered Features\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words or Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Based featurization\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model from Google; OPTIONAL depending on your computational resources (the file is ~1 GB)\n",
    "# Also note that this file path assumes that the word vectors are underneath 'data'; you may wish to point to the CSS course repo and change the path\n",
    "# or move the vector file to the project repo \n",
    "\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average word embeddings for a document; use examples from lab to apply this function. You can use also other techniques such as PCA and doc2vec instead.\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in model.vocab]\n",
    "    return np.mean(model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding based featurization\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either use cross-validation or partition your data with training/validation/test sets for this section. Do the following:\n",
    "\n",
    "* Choose a supervised learning algorithm such as logistic regression, random forest etc. \n",
    "* Train six models. For each of the three dataframes you created in the featurization part, train one model to predict whether the author of the tweet is a Democrat or Republican, and a second model to predict whether the author is a Senator or Representative.\n",
    "* Report the accuracy and other relevant metrics for each of these six models.\n",
    "* Choose the featurization technique associated with your best model. Combine those text features with non-text features. Train two more models: (1) A supervised learning algorithm that uses just the non-text features and (2) a supervised learning algorithm that combines text and non-text features. Report accuracy and other relevant metrics. \n",
    "\n",
    "If time permits, you are encouraged to use hyperparameter tuning or AutoML techniques like TPOT, but are not explicitly required to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Six Models with Just Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# six models ([engineered features, frequency-based, embedding] * [democrat/republican, senator/representative])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Combined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two models ([best text features + non-text features] * [democrat/republican, senator/representative])\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do standard preprocessing techniques need to be further customized to a particular corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Did you find evidence for the idea that Democrats and Republicans have different sentiments in their tweets? What about Senators and Representatives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Why is validating your exploratory and unsupervised learning approaches with a supervised learning algorithm valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Did text only, non-text only, or text and non-text features together perform the best? What is the intuition behind combining text and non-text features in a supervised learning algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE** ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
