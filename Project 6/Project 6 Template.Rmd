---
title: 'Project 6: Randomization and Matching'
student: Brenda Sciepura
date: "`r format(Sys.time(), '%B %d, %Y, %H:%M')`"
output: pdf_document
---

# Introduction

In this project, you will explore the question of whether college education causally affects political participation. Specifically, you will use replication data from \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1409483}{Who Matches? Propensity Scores and Bias in the Causal Eï¬€ects of Education on Participation} by former Berkeley PhD students John Henderson and Sara Chatfield. Their paper is itself a replication study of \href{https://www.jstor.org/stable/10.1017/s0022381608080651}{Reconsidering the Effects of Education on Political Participation} by Cindy Kam and Carl Palmer. In their original 2008 study, Kam and Palmer argue that college education has no effect on later political participation, and use the propensity score matching to show that pre-college political activity drives selection into college and later political participation. Henderson and Chatfield in their 2011 paper argue that the use of the propensity score matching in this context is inappropriate because of the bias that arises from small changes in the choice of variables used to model the propensity score. They use \href{http://sekhon.berkeley.edu/papers/GenMatch.pdf}{genetic matching} (at that point a new method), which uses an approach similar to optimal matching to optimize Mahalanobis distance weights. Even with genetic matching, they find that balance remains elusive however, thus leaving open the question of whether education causes political participation.

You will use these data and debates to investigate the benefits and pitfalls associated with matching methods. Replication code for these papers is available online, but as you'll see, a lot has changed in the last decade or so of data science! Throughout the assignment, use tools we introduced in lab from the \href{https://www.tidyverse.org/}{tidyverse} and the \href{https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf}{MatchIt} packages. Specifically, try to use dplyr, tidyr, purrr, stringr, and ggplot instead of base R functions. While there are other matching software libraries available, MatchIt tends to be the most up to date and allows for consistent syntax.

# Data

The data is drawn from the \href{https://www.icpsr.umich.edu/web/ICPSR/studies/4023/datadocumentation#}{Youth-Parent Socialization Panel Study} which asked students and parents a variety of questions about their political participation. This survey was conducted in several waves. The first wave was in 1965 and established the baseline pre-treatment covariates. The treatment is whether the student attended college between 1965 and 1973 (the time when the next survey wave was administered). The outcome is an index that calculates the number of political activities the student engaged in after 1965. Specifically, the key variables in this study are:

\begin{itemize}
    \item \textbf{college}: Treatment of whether the student attended college or not. 1 if the student attended college between 1965 and 1973, 0 otherwise.
    \item \textbf{ppnscal}: Outcome variable measuring the number of political activities the student participated in. Additive combination of whether the student voted in 1972 or 1980 (student\_vote), attended a campaign rally or meeting (student\_meeting), wore a campaign button (student\_button), donated money to a campaign (student\_money), communicated with an elected official (student\_communicate), attended a demonstration or protest (student\_demonstrate), was involved with a local community event (student\_community), or some other political participation (student\_other)
\end{itemize}

Otherwise, we also have covariates measured for survey responses to various questions about political attitudes. We have covariates measured for the students in the baseline year, covariates for their parents in the baseline year, and covariates from follow-up surveys. \textbf{Be careful here}. In general, post-treatment covariates will be clear from the name (i.e. student\_1973Married indicates whether the student was married in the 1973 survey). Be mindful that the baseline covariates were all measured in 1965, the treatment occurred between 1965 and 1973, and the outcomes are from 1973 and beyond. We will distribute the Appendix from Henderson and Chatfield that describes the covariates they used, but please reach out with any questions if you have questions about what a particular variable means.

```{r, echo=FALSE, warning = FALSE}
# libraries
xfun::pkg_attach2(c("tidyverse",
                    "dplyr", 
                    "here",      
                    "MatchIt",  
                    "gridExtra",
                    "ggplot2", 
                    "optmatch",  
                    "cobalt"))   

options(scipen = 999)

# loading data
ypsps <- read_csv('/Users/brenda/github/Computational-Social-Science-Projects/Project 6/ypsps.csv')

head(ypsps)
nrow(ypsps)

## using simulations from R script "simulations"
source('/Users/brenda/github/Computational-Social-Science-Projects/Project 6/simulations.R')
```

# Randomization

Matching is usually used in observational studies to approximate random assignment to treatment. But could it be useful even in randomized studies? To explore the question do the following:

\begin{enumerate}
    \item Generate a vector that randomly assigns each unit to either treatment or control
    \item Choose a baseline covariate (for either the student or parent). A binary covariate is probably best for this exercise.
    \item Visualize the distribution of the covariate by treatment/control condition. Are treatment and control balanced on this covariate?
    \item Simulate the first 3 steps 10,000 times and visualize the distribution of treatment/control balance across the simulations.
\end{enumerate}

(1) Generate a vector that randomly assigns each unit to treatment/control 

```{r, warning = FALSE}
## set seed 
set.seed(42) 
ypsps$treatment <- sample(c("Control","Treatment"), nrow(ypsps), 
                          replace = TRUE, prob = c(0.50,0.50))
table(ypsps$treatment)
```

(2) Choose a baseline covariate for either the student or the parent and (3) Visualize the distribution of the covariate by treatment/control condition. 
```{r, warning = FALSE}
# Baseline covariate: student_button
table(ypsps$student_button)

# Visualize the distribution by treatment/control (ggplot)
prop_table <- ypsps %>%
  group_by(treatment) %>%
  summarise(prop_student_button = mean(student_button, na.rm = TRUE))

# Bar plot to visualize covariate distribution by treatment group
ggplot(prop_table, aes(x = treatment, y = prop_student_button, fill = treatment, 
                       label = scales::percent(prop_student_button))) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(position = position_dodge(width = 0.9), vjust = -0.5) +
  labs(title = "Proportion of students who wear a button by treatment status",
       x = NULL, 
       y = "Proportion of students who wear a button") +
  scale_y_continuous(labels = scales::percent_format(scale = 100),
                     limits = c(0, 1)) +
  theme_classic() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5))

## test whether the difference between treatment and control it's significant and it's not 
chisq.test(table(ypsps$treatment, ypsps$student_button))
```

**Are treatment/control balanced?**

At baseline, students who wear a button or post to any campaign sites are balanced between the treatment and control group (28% vs 30%, p-value = 0.417). 

(4) Simulate the first three steps 10,000 times and visualize the distribution of treatment/control balance across simulations. 

```{r, warning = FALSE}

# Function to simulate samples, perform t-test, and store results
simulate_and_visualize <- function(ypsps, num_simulations = 10000) {
  
  # Initialize empty vectors to store results
  p_values <- numeric(num_simulations)
  diff_means <- numeric(num_simulations)
  
  # Simulate samples, perform t-test, and store results
  for (i in 1:num_simulations) {
    # Draw sample from df
    ypsps$treatment <- sample(c("Control", "Treatment"), nrow(ypsps), 
                              replace = TRUE, prob = c(0.5, 0.5))
    
    # Perform t-test
    t_test_result <- t.test(ypsps$student_button ~ ypsps$treatment)
    
    # Extract p-value and difference in means from t-test result
    p_values[i] <- t_test_result$p.value
    diff_means[i] <- t_test_result$estimate[2] - t_test_result$estimate[1]
    
}
  
# Create a dataframe to store simulation results
  results_df <- data.frame(
    simulation = 1:num_simulations,
    p_value = p_values,
    diff_in_means = diff_means
  )
  
  # Return the results dataframe
  return(results_df)
}

simulation_results <- simulate_and_visualize(ypsps, num_simulations = 10000)
head(simulation_results, 20)

# Estimate mean of diff in means across simulatins
summary(simulation_results$diff_in_means, na.rm = T) 

# Plot distribution of diff-in-means across simulations
ggplot(simulation_results, aes(x = diff_in_means)) +
  geom_density(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Mean Difference",
       x = "Mean Difference",
       y = "Density")

# Estimate and print proportion of significant p-values across simulations
sum(simulation_results$p_value < 0.05) / nrow(simulation_results) ## 5% 
```

## Questions
\begin{enumerate}
    \item \textbf{What do you see across your simulations? Why does independence of treatment assignment and baseline covariates not guarantee balance of treatment assignment and baseline covariates?}
\end{enumerate}

We observe that the difference in means between the treatment and control changes from simulation to simulations. For instance, the minimum diff-in-means of -0.90 whereas the maximum is 0.13, so we can see a spectrum even if the mean is almost zero -0.0001614. This means that some simulations, just randomly, will get samples that are a bit more different than others, and this is more likely to happen in small samples. The difference in means between them is statistically significant in 5% of the simulations ran, which seems consistent with statistics. 

# Propensity Score Matching

## One Model
Select covariates that you think best represent the "true" model predicting whether a student chooses to attend college, and estimate a propensity score model to calculate the Average Treatment Effect on the Treated (ATT). Plot the balance of the top 10 (or fewer if you select fewer covariates). Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.

```{r, warning = FALSE}

covariates_logistic <- c("student_GPA", "student_FamTalk", 
                         "student_Newspaper", "parent_Vote", 
                         "parent_HHInc", "parent_Employ", "parent_OwnHome")

formula_logistic <- as.formula(paste("college ~", 
                                     paste(covariates_logistic, collapse = " + ")))

match_nearest <- matchit(formula = formula_logistic, 
                         data = ypsps,             
                         method = "nearest",
                         distance = "glm",           
                         link = "logit",             
                         discard = "control",
                         replace = FALSE, 
                         ratio = 2)
bal.tab(match_nearest, un = TRUE, stats = c("m", "v", "ks"))

# estimate ATT
matching_att <- match.data(match_nearest)
dependent_variable <- "student_ppnscal"
independent_variables <- c("college", covariates_logistic)
formula_att <- as.formula(paste(dependent_variable, "~", 
                                paste(independent_variables, collapse = " + ")))

lm_att <- lm(formula = formula_att,
                      data = matching_att,
                      weights = weights)

## get ATT
ATT_nearest <- coef(lm_att)["college"] # Extract ATT
ATT_nearest
    
# Plot the balance for the top 10 covariates
plot(summary(match_nearest))
## love.plot(match_nearest)
## I cannot make this package work, I get the error: 
## Error in scale_override_call(call): could not find function "scale_override_call". 

# Report the balance of the p-scores across both the treatment and control groups, 
## and using a threshold of standardized mean difference of p-score <= .1, 
## report the number of covariates that meet the balance threshold
summary(match_nearest)$sum.matched %>% as.data.frame() 
```

Report the balance of the p-scores across both the treatment and control groups, and using a threshold of standardized mean difference of p-score $\leq .1$, report the number of covariates that meet that balance threshold.

```{r, warning = FALSE}

summary_df <- summary(match_nearest) 
df <- as.data.frame(summary_df$sum.matched)

## report the balance of the p-scores across both the treatment and control groups,
ypsps$pscore <- match_nearest$distance
ypsps %>% group_by(college) %>% summarize(mean(pscore))

## Using a threshold of standardized mean difference of p-score $\leq .1$, 
## report the number of covariates that meet that balance threshold.
num_balanced_covariates  <- nrow(df %>% filter(`Std. Mean Diff.` <= 0.1))
num_balanced_covariates ## 3
```

## Simulations

Henderson/Chatfield argue that an improperly specified propensity score model can actually \textit{increase} the bias of the estimate. To demonstrate this, they simulate 800,000 different propensity score models by choosing different permutations of covariates. To investigate their claim, do the following:

\begin{itemize}
    \item Using as many simulations as is feasible (at least 10,000 should be ok, more is better!), randomly select the number of and the choice of covariates for the propensity score model.
    \item For each run, store the ATT, the proportion of covariates that meet the standardized mean difference $\leq .1$ threshold, and the mean percent improvement in the standardized mean difference. You may also wish to store the entire models in a list and extract the relevant attributes as necessary.
    \item Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.
    \item Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)
\end{itemize}

\textbf{Note: There are lots of post-treatment covariates in this dataset (about 50!)! You need to be careful not to include these in the pre-treatment balancing. Many of you are probably used to selecting or dropping columns manually, or positionally. However, you may not always have a convenient arrangement of columns, nor is it fun to type out 50 different column names. Instead see if you can use dplyr 1.0.0 functions to programatically drop post-treatment variables (\href{https://www.tidyverse.org/blog/2020/03/dplyr-1-0-0-select-rename-relocate/}{here} is a useful tutorial).}

```{r, warning = FALSE}
## I do the first part with the simulation in another R script and import the function here 
head(result_nearest$results_df,10)

## Number of simulations for which the proportion of balanced covariates is above 70%
sum(result_nearest$results_df$prop_balanced_covariates > 0.70)

## Number of simulations for which the proportion of balanced covariates is above 60%
sum(result_nearest$results_df$prop_balanced_covariates > 0.60)

## Number of simulations for which the proportion of balanced covariates is above 50%
sum(result_nearest$results_df$prop_balanced_covariates > 0.50)

print("ATT summary across simulations for results_nearest")
summary(result_nearest$results_df$att)

# Plotting distribution of ATT across simulations
ggplot(result_nearest$results_df, aes(x = att)) +
  geom_density(fill = "skyblue", color = "black") +
  labs(title = "Density Plot of ATT", x = "ATT") +
  theme_minimal() +
   xlim(-1, 2)
```

Plot all of the ATTs against all of the balanced covariate proportions. You may randomly sample or use other techniques like transparency if you run into overplotting problems. Alternatively, you may use plots other than scatterplots, so long as you explore the relationship between ATT and the proportion of covariates that meet the balance threshold.

```{r, warning = FALSE}
# Create a data frame for plotting
plot_data <- data.frame(
  ATT = result_nearest$results_df$att,
  Balanced_Covariate_Proportion = result_nearest$results_df$prop_balanced_covariates  
)

# Create a scatterplot 
ggplot(plot_data, aes(x = ATT, y = Balanced_Covariate_Proportion)) +
  geom_point(alpha = 0.5) +
  labs(x = "ATT", y = "Proportion of Balanced Covariates") +
  ggtitle("Scatterplot of Proportion of Balanced Covariates vs. ATT") +
  xlim(0, 1.5) +  
  ylim(0, 1.5) +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```
Finally choose 10 random models and plot their covariate balance plots (you may want to use a library like \href{https://cran.r-project.org/web/packages/gridExtra/index.html}{gridExtra} to arrange these)

```{r, warning = FALSE}
# Randomly sample 10 indices from the list of models
sampled_indices <- sample(seq_along(result_nearest$match_nearest), size = 10, 
                          replace = FALSE)
random_models <- result_nearest$match_nearest[sampled_indices]

# Function to plot the balance of covariates in each model
plot_summary <- function(model) {
  plot(summary(model))
}

# Apply the plot_summary function to each model in random_models
plots_list <- lapply(random_models, plot_summary)

## I read that supposedly extragrid converts your plots to grobs (https://stackoverflow.com/questions/36901632/r-change-list-of-ggplot-objects-into-a-list-of-grobs-that-grid-arrange-will-ac), but it doesn't. I couldn't figure it out. This code below doesn't work. 
##grid.arrange(grobs = plots_list)
##grid.arrange(grobs = plots_list, ncol = 10) 
```

## Questions

\begin{enumerate}
    \item \textbf{How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?}. 
    Half of the models resulted in a proportion of balanced covariates of 37%, which seems low. The maximum proportion of balanced covariates is 71%. 136 simulations resulted in a proportion above 50% and only 16 in a proportion above 60%. 
    \item \textbf{Analyze the distribution of the ATTs. Do you have any concerns about this distribution?}. 
    Yes, we observe that the ATT varies, as in some simulations it is negative and in other simulations it goes above 1. So different random simulations would give you very different views about the relationships of the variables you're analyzing. 
    \item \textbf{Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?}. 
     Not necessarily. For example for the covariate parent CivicOrg they do, but for the covariate student Radio they are very different in different plots. I guess the post-matching balance for each covariate is conditional on the set of covariates that you choose in the model. I think maybe this is why context knowledge is important, and that ultimately you choose variables that make sense theory-wise.  
\end{enumerate}

# Matching Algorithm of Your Choice

## Simulate Alternative Model

Henderson/Chatfield propose using genetic matching to learn the best weights for Mahalanobis distance matching. Choose a matching algorithm other than the propensity score (you may use genetic matching if you wish, but it is also fine to use the greedy or optimal algorithms we covered in lab instead). Repeat the same steps as specified in Section 4.2 and answer the following questions:

```{r, warning = FALSE}
head(result_optimal$results_df, 10)

# Print summary for result_nearest
print("Summary proportion of balanced covariates for result_nearest:")
summary(result_nearest$results_df$prop_balanced_covariates)

# Print summary for result_optimal
print("Summary proportion of balanced covariates for result_optimal:")
summary(result_optimal$results_df$prop_balanced_covariates)

# Print summary for result_nearest
print("Summary or mean percent improvement for result_nearest:")
print(summary(result_nearest$results_df$mean_pct_improvement))

# Print summary for result_optimal
print("Summary of mean percent improvement for result_optimal:")
print(summary(result_optimal$results_df$mean_pct_improvement))

# Randomly sample 10 indices from the list of models
sampled_indices <- sample(seq_along(result_optimal$match_optimal), size = 10, 
                          replace = FALSE)
random_models <- result_optimal$match_optimal[sampled_indices]

# Function to plot the balance of covariates in each model
plot_summary <- function(model) {
  plot(summary(model))
}

# Apply the plot_summary function to each model in random_models
plots_list <- lapply(random_models, plot_summary)
```

## Questions

\begin{enumerate}
    \item \textbf{Does your alternative matching method have more runs with higher proportions of balanced covariates?}. 
     No, both methods show an average across simulations of 36% of covariates meeting the standardized mean difference threshold. 
    \item \textbf{Use a visualization to examine the change in the distribution of the percent improvement in balance in propensity score matching vs. the distribution of the percent improvement in balance in your new method. Which did better? Analyze the results in 1-2 sentences.}. 
    The mean mean percent improvement across simulations is basically the same (36%) for both methods. 
\end{enumerate}

\textbf{Optional:} Looking ahead to the discussion questions, you may choose to model the propensity score using an algorithm other than logistic regression and perform these simulations again, if you wish to explore the second discussion question further.

# Discussion Questions

\begin{enumerate}
    \item \textbf{Why might it be a good idea to do matching even if we have a randomized or as-if-random design?}. 
    Matching can help reduce the influence of confounding variables, because by matching you make these variables balanced across the treatment and control group. It can also achieve more efficient estimates by reducing variability within matched pairs. By knowing that you're actually comparing someone in the treatment group who's very similar to someone in the control group, it might be easier for a non-academic audience to understand what you're doing and interpreting results. You could also use matching as a robustness check that your results are valid. Also, even if you have a randomized study, sometimes randomization is not perfect, and matching can help control for selection bias. 
    \item \textbf{The standard way of estimating the propensity score is using a logistic regression to estimate probability of treatment. Given what we know about the curse of dimensionality, do you think there might be advantages to using other machine learning algorithms (decision trees, bagging/boosting forests, ensembles, etc.) to estimate propensity scores instead?}. 
    Yes. In propensity score matching, you're selecting a number of covariates to predict the likelihood that someone would get treated. This selection has to be smart, and here is where maching learning could make a difference. Logistic regression, which is what is generally used to estimate the propensity score, addresses linear relationships, so for instance machine learning methods can help model nonlinear relationships between covariates and treatment assignment. In this exercise we selected covariates arbitrarily first and then we ran simulations with different random groups of covariates. In this case, machine learning models can perform feature selection and regularization techniques and keep the most relevant variables to estimate the propensity score. 
\end{enumerate}